{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Picklefile1 = open('dictionary_high_postings', 'rb')\n",
    "dict_high_posting = pickle.load(Picklefile1)\n",
    "\n",
    "Picklefile2 = open('dictionary_low_postings', 'rb')\n",
    "dict_low_posting = pickle.load(Picklefile2)\n",
    "\n",
    "Picklefile3 = open('dictionary_counters', 'rb')\n",
    "dict_counters = pickle.load(Picklefile3)\n",
    "\n",
    "Picklefile4 = open('unique_words_corpus', 'rb')\n",
    "list_unique_words_corpus = pickle.load(Picklefile4)\n",
    "len_list_unique_words_corpus = len(list_unique_words_corpus)\n",
    "\n",
    "Picklefile5 = open('document_names_all', 'rb')\n",
    "all_doc_ids = pickle.load(Picklefile5)\n",
    "len_all_doc_ids = len(all_doc_ids)\n",
    "\n",
    "Picklefile6 = open('idf_dictionary', 'rb')\n",
    "dict_idf = pickle.load(Picklefile6)\n",
    "\n",
    "Picklefile7 = open('doc_quality_scores', 'rb')\n",
    "dict_quality_score = pickle.load(Picklefile7)\n",
    "\n",
    "Picklefile8 = open('nt_maximum', 'rb')\n",
    "max_nt = pickle.load(Picklefile8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the query:\n",
      "BMW with the rear wheel spinning wildly and someone groping for the kill switch\n",
      "Enter the value of k:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter the query:\")\n",
    "query  = input()\n",
    "print(\"Enter the value of k:\")\n",
    "k_value = int(input())\n",
    "#Applying text pre-processing on the query also\n",
    "text = query\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "text = re.sub(r'[a-zA-Z]+[0-9]+', '', text)\n",
    "text = re.sub(r'[0-9]+[a-zA-Z]+', '', text)\n",
    "translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "text = text.translate(translator)\n",
    "word_tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "text = [word for word in word_tokens if word not in stop_words]\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in text]\n",
    "list_query_tokens = copy.deepcopy(lemmas)\n",
    "# print(list_query_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for each query term extracting the posting list\n",
    "list_query_postings = []\n",
    "for tk in list_query_tokens:\n",
    "    list_post = dict_high_posting.get(tk)\n",
    "    if list_post is not None:\n",
    "        list_query_postings.extend(list_post)\n",
    "\n",
    "set_query_postings = set(list_query_postings)\n",
    "list_retrieved_postings = list(set_query_postings)\n",
    "\n",
    "dict_counts_query_tokens = Counter(list_query_tokens)\n",
    "list_query_tokens_freq = list(dict_counts_query_tokens.values())\n",
    "max_freq_query = max(list_query_tokens_freq)\n",
    "\n",
    "vector_query = np.zeros(len_list_unique_words_corpus)\n",
    "for i in range(len_list_unique_words_corpus):\n",
    "    wrd = list_unique_words_corpus[i]\n",
    "    idf_val = dict_idf[wrd]\n",
    "    term_freq = dict_counts_query_tokens.get(wrd)\n",
    "    if term_freq is None:\n",
    "        term_freq = 0\n",
    "    #tf_val = math.log10(1 + term_freq)\n",
    "    tf_val = 0.5 + 0.5 * (term_freq / max_freq_query)\n",
    "    val_product = idf_val * tf_val\n",
    "    vector_query[i] = val_product\n",
    "\n",
    "# print(vector_query)\n",
    "# ct = 0\n",
    "# for j in range(len_list_unique_words_corpus):\n",
    "#     v = vector_query[j]\n",
    "#     if v != 0:\n",
    "#         ct += 1\n",
    "#         print(list_unique_words_corpus[j])\n",
    "# print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 documents in decreasing order of net scores are\n",
      "1.  rec.motorcycles\\105136\t\t1.3265085745609821\n",
      "2.  rec.autos\\103243\t\t1.3060983927387824\n",
      "3.  talk.politics.mideast\\77207\t\t1.2856906341754577\n",
      "4.  rec.autos\\102836\t\t1.285686475407116\n",
      "5.  comp.windows.x\\66871\t\t1.2652817084490806\n",
      "6.  rec.autos\\102770\t\t1.265275611351158\n",
      "7.  rec.autos\\102816\t\t1.265273047306603\n",
      "8.  alt.atheism\\53343\t\t1.265272228004764\n",
      "9.  rec.motorcycles\\105237\t\t1.2448765254364398\n",
      "10.  rec.motorcycles\\104844\t\t1.2448760386455795\n"
     ]
    }
   ],
   "source": [
    "dict_net_scores = {}\n",
    "list_pairs_net_scores = []\n",
    "\n",
    "for doc in list_retrieved_postings:\n",
    "    vector_doc = np.zeros(len_list_unique_words_corpus)\n",
    "    val_gd = dict_quality_score[doc]\n",
    "    curr_dict = dict_counters[doc]\n",
    "    list_doc_tokens_freq = list(curr_dict.values())\n",
    "    max_freq_doc = max(list_doc_tokens_freq)\n",
    "    \n",
    "    for i in range(len_list_unique_words_corpus):\n",
    "        wrd = list_unique_words_corpus[i]\n",
    "        idf_val = dict_idf[wrd]\n",
    "        term_freq = curr_dict.get(wrd)\n",
    "        if term_freq is None:\n",
    "            term_freq = 0\n",
    "#         tf_val = math.log10(1 + term_freq)\n",
    "        tf_val = 0.5 + 0.5 * (term_freq / max_freq_doc)\n",
    "        val_product = idf_val * tf_val\n",
    "        vector_doc[i] = val_product\n",
    "    \n",
    "    norm_q_vector = np.linalg.norm(vector_query)\n",
    "    norm_doc_vector = np.linalg.norm(vector_doc)\n",
    "    cosine_sim = (np.dot(vector_query, vector_doc)) / (norm_q_vector * norm_doc_vector)\n",
    "    net_score = val_gd + cosine_sim\n",
    "    pair = [doc, net_score]\n",
    "    list_pairs_net_scores.append(pair)\n",
    "\n",
    "list_pairs_net_scores.sort(key= lambda x:x[1], reverse=True)\n",
    "\n",
    "len_final_retrieved = 0\n",
    "if len(list_pairs_net_scores) < k_value:\n",
    "    len_final_retrieved = len(list_pairs_net_scores)\n",
    "else:\n",
    "    len_final_retrieved = k_value\n",
    "\n",
    "\n",
    "list_final_retrieved = []\n",
    "\n",
    "for ab in range(len_final_retrieved):\n",
    "    list_final_retrieved.append(list_pairs_net_scores[ab])\n",
    "\n",
    "print(f\"The top {len_final_retrieved} documents in decreasing order of net scores are\")\n",
    "\n",
    "\n",
    "for ad in range(len_final_retrieved):\n",
    "    print(f\"{ad+1}.  {list_final_retrieved[ad][0]}\\t\\t{list_final_retrieved[ad][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
