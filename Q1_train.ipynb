{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "93107\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "files = []\n",
    "directory = r'D:\\M.TECH SEM 2\\IR\\Assignments\\A3\\20_newsgroups\\\\'\n",
    "for entry in os.listdir(directory):\n",
    "    if os.path.isdir(os.path.join(directory, entry)):\n",
    "        files.append(entry)\n",
    "\n",
    "docs = []\n",
    "directory = r'D:\\M.TECH SEM 2\\IR\\Assignments\\A3\\20_newsgroups\\\\'\n",
    "dict_counters = {}\n",
    "dict_postings = {}\n",
    "corpus_words = []\n",
    "ctr = 1\n",
    "all_doc_ids = []\n",
    "\n",
    "for fol in files:\n",
    "    print(fol)\n",
    "    temp_dir = os.path.join(directory, fol)\n",
    "    docs = []\n",
    "    complete_doc_loc = []\n",
    "    for entry in os.listdir(temp_dir):\n",
    "        if os.path.isfile(os.path.join(temp_dir, entry)):\n",
    "            docs.append(entry)\n",
    "            doc_loc = os.path.join(fol, entry)\n",
    "            complete_doc_loc.append(doc_loc)\n",
    "\n",
    "    all_doc_ids.extend(complete_doc_loc)\n",
    "    for doc in docs:\n",
    "        doc_loc = os.path.join(fol, doc)\n",
    "        full_path = os.path.join(temp_dir, doc)\n",
    "        fp = open(full_path, \"r\")\n",
    "        text = fp.read()\n",
    "        fp.close()\n",
    "        ll = text.split(\"\\n\\n\")\n",
    "        del ll[0]\n",
    "        text = \"\\n\\n\".join(ll)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "        text = re.sub(r'[a-zA-Z]+[0-9]+', ' ', text)\n",
    "        text = re.sub(r'[0-9]+[a-zA-Z]+', ' ', text)\n",
    "        text = re.sub(r'[a-zA-Z]+[0-9]+[a-zA-Z]+', ' ', text)\n",
    "        text = re.sub(r'[0-9]+[a-zA-Z]+[0-9]+', ' ', text)\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        text = text.translate(translator)\n",
    "        word_tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [word for word in word_tokens if word not in stop_words]\n",
    "        lemmas = [lemmatizer.lemmatize(word) for word in text]\n",
    "        list_words = copy.deepcopy(lemmas)\n",
    "        dict_counts = Counter(list_words)\n",
    "        dict_counters[doc_loc] = dict_counts\n",
    "        unique_words = []\n",
    "        for k in dict_counts:\n",
    "            unique_words.append(k)\n",
    "        corpus_words.extend(unique_words)\n",
    "\n",
    "        for wd in unique_words:\n",
    "            if dict_postings.get(wd) is None:\n",
    "                list_dc = [doc_loc]\n",
    "                dict_postings[wd] = list_dc\n",
    "            else:\n",
    "                dict_postings[wd].append(doc_loc)\n",
    "\n",
    "# dict_counts_corpus_words = Counter(corpus_words)\n",
    "# list_unique_words_corpus = []\n",
    "#Making a list of all unique words in the corpus\n",
    "# for kk in dict_counts_corpus_words:\n",
    "#     list_unique_words_corpus.append(kk)\n",
    "\n",
    "\n",
    "# for wd in list_unique_words_corpus:\n",
    "#     for dc in dict_counters:\n",
    "#         curr_cntr = dict_counters[dc]\n",
    "#         if curr_cntr[wd] != 0:\n",
    "#             if dict_postings.get(wd) is None:\n",
    "#                 list_dc = [dc]\n",
    "#                 dict_postings[wd] = list_dc\n",
    "#             else:\n",
    "#                 dict_postings[wd].append(dc)\n",
    "\n",
    "print(len(dict_postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_doc_hits = {}\n",
    "dict_idf = {}\n",
    "list_hits = []\n",
    "for k in dict_postings:\n",
    "    list_posting = dict_postings[k]\n",
    "    dict_doc_hits[k] = len(list_posting)\n",
    "    list_hits.append(len(list_posting))\n",
    "    \n",
    "max_nt = max(list_hits)\n",
    "for wu in dict_doc_hits:\n",
    "    idf_val = math.log10(max_nt / (1 + dict_doc_hits[wu]))\n",
    "    dict_idf[wu] = idf_val\n",
    "\n",
    "# print(len(dict_idf))\n",
    "# print(max_nt)\n",
    "# print(dict_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_corpus_words = Counter(corpus_words)\n",
    "# dict_doc_hits = {}\n",
    "# dict_idf = {}\n",
    "# list_unique_words_corpus = []\n",
    "# for wr in dict_corpus_words:\n",
    "#     list_unique_words_corpus.append(wr)\n",
    "\n",
    "# for wrd in list_unique_words_corpus:\n",
    "#     # dict_doc_hits[wrd] = 0\n",
    "#     for ki in dict_counters:\n",
    "#         curr_dict = dict_counters[ki]\n",
    "#         if curr_dict[wrd] != 0:\n",
    "#             if dict_doc_hits.get(wrd) is None:\n",
    "#                 dict_doc_hits[wrd] = 1\n",
    "#             else:\n",
    "#                 dict_doc_hits[wrd] = dict_doc_hits[wrd] + 1\n",
    "\n",
    "# list_doc_hits = []\n",
    "# for k in dict_doc_hits:\n",
    "#     list_doc_hits.append(dict_doc_hits[k])\n",
    "\n",
    "# max_nt = max(list_doc_hits)\n",
    "# print(f\"max nat val = {max_nt}\")\n",
    "# for word in dict_doc_hits:\n",
    "#     idf_val = math.log10(max_nt / (1 + dict_doc_hits[word]))\n",
    "#     dict_idf[word] = idf_val\n",
    "\n",
    "# print(len(dict_idf))\n",
    "# print(dict_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median doc freq = 14.0\n",
      "\n",
      "\n",
      "\n",
      "Counter({1: 41890, 2: 13672, 3: 6724, 4: 4301, 5: 3182, 6: 2246, 7: 1722, 8: 1388, 9: 1215, 10: 998, 11: 916, 12: 779, 13: 705, 15: 581, 14: 575, 16: 525, 17: 460, 19: 402, 18: 400, 21: 335, 20: 330, 22: 294, 23: 285, 24: 262, 26: 232, 25: 226, 27: 208, 28: 199, 30: 191, 29: 172, 31: 169, 34: 161, 32: 158, 35: 150, 33: 149, 36: 137, 37: 123, 43: 123, 40: 119, 41: 116, 38: 111, 39: 111, 42: 103, 45: 102, 47: 101, 46: 100, 50: 84, 44: 84, 52: 77, 53: 77, 49: 73, 48: 73, 54: 72, 56: 68, 51: 67, 55: 62, 58: 57, 69: 56, 61: 54, 64: 54, 57: 53, 67: 51, 63: 50, 72: 50, 59: 50, 73: 49, 62: 49, 60: 46, 74: 46, 75: 46, 66: 45, 80: 42, 79: 41, 65: 41, 88: 40, 68: 40, 93: 38, 89: 38, 76: 38, 78: 37, 77: 37, 82: 36, 98: 35, 71: 34, 91: 33, 90: 33, 84: 32, 70: 32, 83: 32, 86: 30, 81: 30, 99: 30, 85: 30, 87: 30, 102: 29, 95: 29, 105: 28, 111: 28, 94: 28, 101: 27, 126: 24, 139: 23, 107: 23, 110: 23, 114: 23, 96: 23, 104: 23, 97: 23, 121: 22, 109: 22, 158: 22, 141: 21, 113: 21, 108: 21, 112: 20, 116: 20, 103: 20, 100: 20, 92: 20, 115: 20, 134: 19, 120: 19, 137: 19, 151: 19, 157: 18, 119: 18, 154: 18, 135: 18, 143: 18, 117: 17, 130: 17, 124: 16, 106: 16, 144: 16, 186: 16, 123: 16, 122: 16, 129: 16, 128: 16, 148: 15, 231: 15, 118: 15, 127: 15, 205: 15, 175: 15, 140: 14, 210: 14, 132: 14, 188: 14, 160: 14, 145: 14, 162: 14, 165: 14, 197: 14, 146: 14, 176: 13, 156: 13, 138: 13, 142: 13, 174: 13, 232: 13, 147: 13, 136: 13, 181: 13, 163: 13, 178: 12, 189: 12, 172: 12, 214: 12, 166: 12, 199: 11, 191: 11, 190: 11, 173: 11, 150: 11, 149: 11, 234: 11, 238: 11, 182: 11, 167: 11, 187: 10, 208: 10, 201: 10, 177: 10, 133: 10, 159: 10, 171: 10, 225: 10, 221: 9, 236: 9, 280: 9, 230: 9, 207: 9, 194: 9, 168: 9, 161: 9, 200: 9, 219: 9, 125: 9, 184: 9, 263: 9, 192: 9, 185: 8, 202: 8, 243: 8, 325: 8, 365: 8, 223: 8, 164: 8, 152: 8, 216: 8, 218: 8, 155: 8, 259: 8, 153: 8, 131: 7, 247: 7, 331: 7, 170: 7, 346: 7, 265: 7, 203: 7, 288: 7, 313: 7, 228: 7, 264: 7, 287: 7, 266: 7, 249: 7, 299: 7, 250: 7, 310: 7, 235: 7, 342: 6, 372: 6, 281: 6, 213: 6, 224: 6, 367: 6, 324: 6, 269: 6, 258: 6, 252: 6, 222: 6, 307: 6, 180: 6, 312: 6, 204: 6, 332: 6, 226: 6, 256: 6, 211: 6, 193: 6, 169: 6, 206: 6, 198: 6, 179: 6, 242: 6, 196: 5, 314: 5, 366: 5, 298: 5, 296: 5, 272: 5, 334: 5, 423: 5, 377: 5, 338: 5, 284: 5, 384: 5, 341: 5, 422: 5, 322: 5, 260: 5, 465: 5, 215: 5, 209: 5, 220: 5, 285: 5, 364: 5, 195: 5, 319: 5, 257: 5, 212: 5, 261: 5, 241: 5, 291: 5, 255: 4, 348: 4, 583: 4, 507: 4, 320: 4, 595: 4, 394: 4, 673: 4, 442: 4, 420: 4, 504: 4, 662: 4, 262: 4, 373: 4, 344: 4, 483: 4, 244: 4, 359: 4, 510: 4, 333: 4, 393: 4, 311: 4, 723: 4, 763: 4, 534: 4, 328: 4, 439: 4, 330: 4, 452: 4, 304: 4, 421: 4, 316: 4, 458: 4, 399: 4, 239: 4, 290: 4, 415: 4, 286: 4, 237: 4, 245: 4, 386: 4, 271: 4, 401: 4, 302: 4, 326: 4, 397: 4, 183: 4, 335: 4, 460: 4, 453: 4, 273: 4, 659: 4, 253: 4, 217: 4, 413: 4, 233: 4, 466: 3, 543: 3, 801: 3, 293: 3, 370: 3, 406: 3, 303: 3, 254: 3, 647: 3, 404: 3, 379: 3, 321: 3, 405: 3, 471: 3, 541: 3, 308: 3, 318: 3, 1344: 3, 382: 3, 283: 3, 411: 3, 419: 3, 410: 3, 563: 3, 528: 3, 347: 3, 509: 3, 329: 3, 921: 3, 270: 3, 519: 3, 597: 3, 786: 3, 267: 3, 972: 3, 687: 3, 586: 3, 418: 3, 376: 3, 357: 3, 556: 3, 278: 3, 473: 3, 400: 3, 627: 3, 414: 3, 387: 3, 340: 3, 503: 3, 337: 3, 428: 3, 552: 3, 248: 3, 229: 3, 574: 3, 358: 3, 398: 3, 440: 3, 757: 3, 570: 3, 383: 3, 362: 3, 407: 3, 433: 3, 246: 3, 275: 3, 227: 3, 306: 3, 360: 3, 778: 2, 456: 2, 789: 2, 1331: 2, 716: 2, 961: 2, 564: 2, 470: 2, 361: 2, 524: 2, 970: 2, 540: 2, 1015: 2, 459: 2, 1306: 2, 657: 2, 773: 2, 766: 2, 1450: 2, 850: 2, 590: 2, 712: 2, 517: 2, 1805: 2, 585: 2, 646: 2, 704: 2, 648: 2, 982: 2, 1317: 2, 496: 2, 490: 2, 498: 2, 468: 2, 566: 2, 425: 2, 469: 2, 343: 2, 436: 2, 1565: 2, 642: 2, 847: 2, 416: 2, 1181: 2, 499: 2, 584: 2, 819: 2, 600: 2, 525: 2, 675: 2, 437: 2, 800: 2, 963: 2, 744: 2, 448: 2, 455: 2, 1040: 2, 605: 2, 620: 2, 702: 2, 1072: 2, 875: 2, 1512: 2, 1237: 2, 658: 2, 474: 2, 924: 2, 279: 2, 953: 2, 866: 2, 292: 2, 871: 2, 677: 2, 1190: 2, 345: 2, 514: 2, 951: 2, 467: 2, 639: 2, 698: 2, 618: 2, 438: 2, 1026: 2, 449: 2, 852: 2, 630: 2, 589: 2, 309: 2, 607: 2, 950: 2, 484: 2, 487: 2, 336: 2, 274: 2, 339: 2, 814: 2, 485: 2, 475: 2, 609: 2, 1234: 2, 368: 2, 392: 2, 569: 2, 565: 2, 434: 2, 798: 2, 835: 2, 516: 2, 562: 2, 268: 2, 297: 2, 389: 2, 765: 2, 461: 2, 531: 2, 823: 2, 388: 2, 402: 2, 349: 2, 686: 2, 721: 2, 596: 2, 728: 2, 608: 2, 651: 2, 495: 2, 689: 2, 251: 2, 354: 2, 317: 2, 546: 2, 890: 2, 385: 2, 412: 2, 276: 2, 816: 2, 435: 2, 561: 2, 560: 2, 427: 2, 390: 2, 363: 2, 301: 2, 443: 2, 395: 2, 1494: 1, 2273: 1, 994: 1, 4754: 1, 1575: 1, 916: 1, 3521: 1, 1564: 1, 986: 1, 6330: 1, 7910: 1, 1106: 1, 1533: 1, 2632: 1, 576: 1, 4594: 1, 1091: 1, 5629: 1, 1844: 1, 1024: 1, 3344: 1, 1030: 1, 567: 1, 559: 1, 1223: 1, 573: 1, 1126: 1, 1146: 1, 1065: 1, 1107: 1, 447: 1, 863: 1, 3493: 1, 3409: 1, 3180: 1, 1208: 1, 1705: 1, 2996: 1, 980: 1, 1315: 1, 691: 1, 350: 1, 663: 1, 1506: 1, 3916: 1, 1619: 1, 1709: 1, 477: 1, 1552: 1, 1900: 1, 1418: 1, 3669: 1, 3791: 1, 1537: 1, 3188: 1, 1517: 1, 4864: 1, 1546: 1, 889: 1, 899: 1, 1678: 1, 947: 1, 568: 1, 2154: 1, 2755: 1, 1965: 1, 1251: 1, 699: 1, 720: 1, 2810: 1, 3292: 1, 935: 1, 300: 1, 578: 1, 2791: 1, 770: 1, 1471: 1, 1728: 1, 479: 1, 472: 1, 3149: 1, 3237: 1, 1842: 1, 1817: 1, 614: 1, 6137: 1, 1636: 1, 4272: 1, 2271: 1, 1286: 1, 1894: 1, 3782: 1, 1985: 1, 462: 1, 808: 1, 2031: 1, 4548: 1, 481: 1, 1617: 1, 3351: 1, 2085: 1, 1732: 1, 2515: 1, 2112: 1, 1345: 1, 3923: 1, 969: 1, 380: 1, 429: 1, 508: 1, 1677: 1, 1392: 1, 861: 1, 1388: 1, 1796: 1, 1426: 1, 1986: 1, 624: 1, 4856: 1, 1473: 1, 1781: 1, 881: 1, 2059: 1, 1387: 1, 2173: 1, 1228: 1, 2376: 1, 2365: 1, 1119: 1, 1087: 1, 830: 1, 2187: 1, 9247: 1, 1353: 1, 1783: 1, 1071: 1, 1069: 1, 2240: 1, 2473: 1, 4511: 1, 944: 1, 1203: 1, 1389: 1, 7707: 1, 792: 1, 2674: 1, 2702: 1, 2880: 1, 1176: 1, 1789: 1, 3525: 1, 775: 1, 409: 1, 2322: 1, 873: 1, 3191: 1, 1412: 1, 571: 1, 3735: 1, 2405: 1, 2778: 1, 940: 1, 853: 1, 277: 1, 2220: 1, 1059: 1, 737: 1, 374: 1, 2587: 1, 2094: 1, 1689: 1, 4094: 1, 2434: 1, 1086: 1, 1036: 1, 4081: 1, 282: 1, 831: 1, 2605: 1, 1276: 1, 2417: 1, 1372: 1, 3071: 1, 1687: 1, 732: 1, 1166: 1, 451: 1, 1758: 1, 2849: 1, 305: 1, 588: 1, 777: 1, 1346: 1, 527: 1, 645: 1, 1017: 1, 1455: 1, 897: 1, 1186: 1, 1437: 1, 893: 1, 711: 1, 794: 1, 1060: 1, 678: 1, 1486: 1, 952: 1, 2299: 1, 665: 1, 957: 1, 1663: 1, 1669: 1, 2168: 1, 1152: 1, 2072: 1, 2033: 1, 1895: 1, 464: 1, 684: 1, 1041: 1, 1354: 1, 1509: 1, 355: 1, 240: 1, 1898: 1, 632: 1, 898: 1, 3163: 1, 1058: 1, 3775: 1, 1279: 1, 511: 1, 809: 1, 753: 1, 1852: 1, 1164: 1, 1462: 1, 1749: 1, 493: 1, 832: 1, 1048: 1, 644: 1, 551: 1, 1468: 1, 931: 1, 1407: 1, 983: 1, 1820: 1, 718: 1, 1995: 1, 444: 1, 3369: 1, 621: 1, 1439: 1, 369: 1, 1335: 1, 776: 1, 1425: 1, 761: 1, 2245: 1, 1160: 1, 964: 1, 1771: 1, 1183: 1, 619: 1, 1377: 1, 3259: 1, 523: 1, 622: 1, 1638: 1, 1622: 1, 1680: 1, 929: 1, 1927: 1, 581: 1, 1429: 1, 1480: 1, 857: 1, 1386: 1, 1005: 1, 774: 1, 1102: 1, 1655: 1, 545: 1, 2190: 1, 2857: 1, 654: 1, 10546: 1, 797: 1, 538: 1, 1168: 1, 623: 1, 2189: 1, 888: 1, 626: 1, 1148: 1, 825: 1, 688: 1, 812: 1, 926: 1, 489: 1, 1134: 1, 743: 1, 536: 1, 785: 1, 1305: 1, 426: 1, 885: 1, 995: 1, 353: 1, 640: 1, 351: 1, 1097: 1, 2074: 1, 1154: 1, 912: 1, 497: 1, 1212: 1, 764: 1, 2231: 1, 690: 1, 671: 1, 1139: 1, 1014: 1, 2562: 1, 2218: 1, 512: 1, 1402: 1, 960: 1, 747: 1, 480: 1, 1293: 1, 1571: 1, 844: 1, 554: 1, 502: 1, 446: 1, 993: 1, 1658: 1, 1568: 1, 1064: 1, 811: 1, 579: 1, 391: 1, 695: 1, 553: 1, 378: 1, 522: 1, 955: 1, 1023: 1, 610: 1, 1535: 1, 683: 1, 526: 1, 749: 1, 839: 1, 616: 1, 895: 1, 734: 1, 848: 1, 1270: 1, 810: 1, 845: 1, 751: 1, 706: 1, 883: 1, 682: 1, 501: 1, 1136: 1, 649: 1, 548: 1, 967: 1, 478: 1, 352: 1, 1659: 1, 613: 1, 1009: 1, 1018: 1, 822: 1, 530: 1, 476: 1, 679: 1, 371: 1, 669: 1, 1006: 1, 1629: 1, 703: 1, 736: 1, 457: 1, 1454: 1, 855: 1, 807: 1, 2097: 1, 750: 1, 506: 1, 685: 1, 396: 1, 557: 1, 1094: 1, 752: 1, 289: 1, 547: 1, 913: 1, 1199: 1, 887: 1, 791: 1, 1032: 1, 539: 1, 577: 1, 575: 1, 593: 1, 806: 1, 1140: 1, 989: 1, 408: 1, 486: 1, 746: 1, 631: 1, 521: 1, 432: 1, 431: 1, 294: 1, 381: 1, 550: 1, 1244: 1, 1144: 1, 356: 1, 315: 1, 824: 1, 756: 1, 529: 1, 971: 1, 625: 1, 430: 1, 1673: 1, 635: 1, 500: 1, 488: 1, 424: 1, 707: 1, 643: 1, 666: 1, 463: 1, 494: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import statistics\n",
    "list_post_length = []\n",
    "list_find_r = []\n",
    "for k in dict_postings:\n",
    "    curr_len = len(dict_postings[k])\n",
    "    list_post_length.append(curr_len)\n",
    "    if curr_len != 1 and curr_len != 2 and curr_len != 3 and curr_len != 4:\n",
    "        list_find_r.append(curr_len)\n",
    "\n",
    "dict_counter_length = Counter(list_post_length)\n",
    "median_doc_freq = statistics.median(list_find_r)\n",
    "print(f\"Median doc freq = {median_doc_freq}\")\n",
    "print(\"\\n\\n\")\n",
    "print(dict_counter_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "dict_quality_score = {}\n",
    "ctr_read_idx = 0\n",
    "\n",
    "directory = r'D:\\M.TECH SEM 2\\IR\\Assignments\\A3\\20_newsgroups\\\\'\n",
    "for entry in os.listdir(directory):\n",
    "    if os.path.isdir(os.path.join(directory, entry)):\n",
    "        files.append(entry)\n",
    "\n",
    "directory = r'D:\\M.TECH SEM 2\\IR\\Assignments\\A3\\20_newsgroups\\\\'\n",
    "fp123 = open(r'D:\\M.TECH SEM 2\\IR\\Assignments\\A3\\file.txt', \"r\")\n",
    "data_rel = fp123.read()\n",
    "lines_data = data_rel.split(\"\\n\")\n",
    "# print(lines_data)\n",
    "list_doc_rel_data = []\n",
    "list_all_quality_scores = []\n",
    "for d in lines_data:\n",
    "    l = d.split()\n",
    "    list_doc_rel_data.append(l)\n",
    "# print(list_doc_rel_data)\n",
    "\n",
    "for fol in files:\n",
    "#     print(fol)\n",
    "    temp_dir = os.path.join(directory, fol)\n",
    "    complete_doc_loc = []\n",
    "    for entry in os.listdir(temp_dir):\n",
    "        if os.path.isfile(os.path.join(temp_dir, entry)):\n",
    "            doc_loc = os.path.join(fol, entry)\n",
    "            complete_doc_loc.append(doc_loc)\n",
    "    \n",
    "    no_of_docs = len(complete_doc_loc)\n",
    "    for ab in range(no_of_docs):\n",
    "        curr_doc = complete_doc_loc[ab]\n",
    "        score_quality = int(list_doc_rel_data[ctr_read_idx][1])\n",
    "        ctr_read_idx += 1\n",
    "        dict_quality_score[curr_doc] = score_quality\n",
    "        list_all_quality_scores.append(score_quality)\n",
    "\n",
    "max_quality_score = max(list_all_quality_scores)\n",
    "for kh in dict_quality_score:\n",
    "    dict_quality_score[kh] = dict_quality_score[kh] / max_quality_score\n",
    "# print(len(dict_quality_score))\n",
    "# print(dict_quality_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93107\n"
     ]
    }
   ],
   "source": [
    "for wd in dict_postings:\n",
    "    list_posting = dict_postings[wd]\n",
    "    idf_val = dict_idf[wd]\n",
    "    dict_tf = {}\n",
    "    for doc in list_posting:\n",
    "        curr_dict = dict_counters[doc]\n",
    "        dict_tf[doc] = curr_dict[wd]\n",
    "#     list_tf = [dict_counters[a][wd] for a in list_posting]\n",
    "    list_tf_dot_idf = []\n",
    "    list_gd_plus_tfidf = []\n",
    "    list_pairs_doc_gd_tfidf = []\n",
    "    for doc in dict_tf:\n",
    "        gd_val = dict_quality_score[doc]\n",
    "        tf_val = dict_tf[doc]\n",
    "        val_tfidf = idf_val * tf_val\n",
    "        val_total = gd_val + val_tfidf\n",
    "        pair = [doc, val_total]\n",
    "        list_pairs_doc_gd_tfidf.append(pair)\n",
    "    \n",
    "    list_pairs_doc_gd_tfidf.sort(key= lambda x: x[1], reverse = True)\n",
    "    \n",
    "    list_sorted_posting = [a for a,b in list_pairs_doc_gd_tfidf]\n",
    "    dict_postings[wd] = list_sorted_posting\n",
    "\n",
    "print(len(dict_postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93107\n",
      "93107\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "value_r = 36\n",
    "dict_high_posting = {}\n",
    "dict_low_posting = {}\n",
    "for wd in dict_postings:\n",
    "    list_posting = dict_postings[wd]\n",
    "    len_posting = len(list_posting)\n",
    "    if len_posting <= value_r:\n",
    "        dict_high_posting[wd] = list_posting\n",
    "        dict_low_posting[wd] = []\n",
    "    else:\n",
    "        dict_high_posting[wd] = list_posting[:value_r - 1]\n",
    "        dict_low_posting[wd] = list_posting[value_r:]\n",
    "\n",
    "print(len(dict_high_posting))\n",
    "print(len(dict_low_posting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# dict_high_posting = {}\n",
    "# dict_low_posting = {}\n",
    "# for wd in dict_postings:\n",
    "#     list_posting = dict_postings[wd]\n",
    "#     len_posting = len(list_posting)\n",
    "#     if len_posting > 1:\n",
    "#         len_high = math.floor(((2.0 / 3.0) * len_posting))\n",
    "#         len_low = len_posting - len_high\n",
    "#     else:\n",
    "#         len_high = math.ceil(((2.0 / 3.0) * len_posting))\n",
    "#         len_low = len_posting - len_high\n",
    "#     dict_high_posting[wd] = list_posting[0:len_high-1]\n",
    "#     if len_low == 0:\n",
    "#         dict_low_posting[wd] = []\n",
    "#     else:\n",
    "#         dict_low_posting[wd] = list_posting[len_high:]\n",
    "\n",
    "# print(len(dict_high_posting))\n",
    "# print(len(dict_low_posting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93107\n",
      "93107\n"
     ]
    }
   ],
   "source": [
    "for k in dict_high_posting:\n",
    "    curr_list = dict_high_posting[k]\n",
    "    curr_quality_scores = [dict_quality_score[a] for a in curr_list]\n",
    "    combined_list = sorted(zip(curr_list, curr_quality_scores), key = lambda x:x[1], reverse = True)\n",
    "    sorted_docs_list = [a for a,b in combined_list]\n",
    "    dict_high_posting[k] = sorted_docs_list\n",
    "\n",
    "for k in dict_low_posting:\n",
    "    curr_list = dict_low_posting[k]\n",
    "    curr_quality_scores = [dict_quality_score[a] for a in curr_list]\n",
    "    combined_list = sorted(zip(curr_list, curr_quality_scores), key = lambda x:x[1], reverse = True)\n",
    "    sorted_docs_list = [a for a,b in combined_list]\n",
    "    dict_low_posting[k] = sorted_docs_list\n",
    "\n",
    "print(len(dict_high_posting))\n",
    "print(len(dict_low_posting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93107\n"
     ]
    }
   ],
   "source": [
    "dict_corpus_words = Counter(corpus_words)\n",
    "list_unique_words_corpus = list(dict_corpus_words.keys())\n",
    "print(len(list_unique_words_corpus))\n",
    "# print(type(list_unique_words_corpus))\n",
    "\n",
    "# matrix_doc_vectors = [[0 for i in tqdm(range(len(list_unique_words_corpus)))] for j in tqdm(range(len(all_doc_ids)))]\n",
    "# cnt_rows = len(all_doc_ids)\n",
    "# cnt_columns = len(list_unique_words_corpus)\n",
    "# matrix_doc_vectors = np.zeros((cnt_rows, cnt_columns), dtype = 'float')\n",
    "\n",
    "# dict_doc_vectors = {}\n",
    "# print(matrix_doc_vectors)\n",
    "\n",
    "# for i in tqdm(range(len(all_doc_ids))):\n",
    "#     curr_doc = all_doc_ids[i]\n",
    "#     curr_dict = dict_counters[curr_doc]\n",
    "#     list_values = []\n",
    "#     for j in tqdm(range(len(list_unique_words_corpus))):\n",
    "#         curr_word = list_unique_words_corpus[j]\n",
    "#         idf_val = dict_idf[curr_word]\n",
    "#         term_freq = curr_dict[curr_word]\n",
    "#         tf_val = math.log10(1 + term_freq)\n",
    "#         list_values.append(idf_val * tf_val)\n",
    "#         matrix_doc_vectors[i][j] = idf_val * tf_val\n",
    "#     dict_doc_vectors[curr_doc] = list_values\n",
    "\n",
    "# print(len(dict_doc_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Picklefile1 = open('dictionary_high_postings', 'wb')\n",
    "pickle.dump(dict_high_posting, Picklefile1)\n",
    "Picklefile1.close()\n",
    "\n",
    "Picklefile2 = open('dictionary_low_postings', 'wb')\n",
    "pickle.dump(dict_low_posting, Picklefile2)\n",
    "Picklefile2.close()\n",
    "\n",
    "Picklefile3 = open('dictionary_counters', 'wb')\n",
    "pickle.dump(dict_counters, Picklefile3)\n",
    "Picklefile3.close()\n",
    "\n",
    "Picklefile4 = open('unique_words_corpus', 'wb')\n",
    "pickle.dump(list_unique_words_corpus, Picklefile4)\n",
    "Picklefile4.close()\n",
    "\n",
    "Picklefile5 = open('document_names_all', 'wb')\n",
    "pickle.dump(all_doc_ids, Picklefile5)\n",
    "Picklefile5.close()\n",
    "\n",
    "Picklefile6 = open('idf_dictionary', 'wb')\n",
    "pickle.dump(dict_idf, Picklefile6)\n",
    "Picklefile6.close()\n",
    "\n",
    "Picklefile7 = open('doc_quality_scores', 'wb')\n",
    "pickle.dump(dict_quality_score, Picklefile7)\n",
    "Picklefile7.close()\n",
    "\n",
    "Picklefile8 = open('nt_maximum', 'wb')\n",
    "pickle.dump(max_nt, Picklefile8)\n",
    "Picklefile8.close()\n",
    "\n",
    "Picklefile9 = open('dictionary_postings', 'wb')\n",
    "pickle.dump(dict_postings, Picklefile9)\n",
    "Picklefile9.close()\n",
    "# Picklefile4 = open('doc', 'wb')\n",
    "# pickle.dump(matrix_doc_vectors, Picklefile4)\n",
    "# Picklefile4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
